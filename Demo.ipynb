{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIG - Task 1. Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We worked with different architectures provided by Keras for text classification and tried them out using several kind of vector representations of words (embeddings).\n",
    "\n",
    "#### Embeddings:\n",
    "\n",
    "- w2v\n",
    "- GloVe\n",
    "- Dependency based embeddings\n",
    "- FastText embeddings\n",
    "- Combination of GloVe with urban dictionary embeddings\n",
    "\n",
    "The most common and used embeddings are w2v and GloVe, however dependency based embeddings are demonstrated to achieve high accuracy. FastText embeddings are compressed vectors that enable a faster training step and our contribution here was to use a combination of GloVe and word embeddings trained on [Urban Dictionary](https://www.urbandictionary.com/). In other words we took 200 dimenssions of GloVe and concatenated them with 100 dimensions of Urban dictionary embeddings as shown in the graph below:\n",
    "![urban_embeddings](../urbandictemb.png)\n",
    "\n",
    "     \n",
    "#### Architectures:\n",
    "\n",
    "All three architectures include an embedding layer that pulls our already mentioned pre-trained embeddings. In the same way we used a dropout rate of 0.2, binary crossentropy as loss function and Adam as optimizer.\n",
    "\n",
    "- CNN: which contains one 1-dimensional convolutional layer and one max-pooling layer\n",
    "- CNN+LSTM: adding to the previous layers an lstm layer\n",
    "- LSTM: containing just an lstm layer\n",
    "\n",
    "### Our best result:\n",
    "\n",
    "Our best score was achieved by the cnn model trained with the dependecy based embeddings proposed by [Komninos, A., & Manandhar, S. (2016)](http://www-users.cs.york.ac.uk/~suresh/papers/dep_embeddings_naacl2016.pdf). Here we're going to see an example with that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lstm_classifier import LstmModel\n",
    "from cnn_classifier import CNN as cnn\n",
    "from cnn_lstm_classifier import CnnLstm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from data_helpers import get_embeddings\n",
    "from predicter import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(data_file, adjust_labels):\n",
    "    # Load data from file\n",
    "    x_text = []\n",
    "    original_y = []\n",
    "\n",
    "    for line in open(data_file, \"r\"):\n",
    "        x_text.append(line.strip().split(\"\\t\")[0])\n",
    "        if adjust_labels:\n",
    "            original_y.append(line.strip().split(\"\\t\")[-1])\n",
    "        else:\n",
    "            original_y.append(int(line.strip().split(\"\\t\")[-1]))\n",
    "    return original_y, x_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_labels(array):\n",
    "\n",
    "    y = np.array(array)\n",
    "    y[y == \"non-propaganda\"] = 0\n",
    "    y[y == \"propaganda\"] = 1\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(x_file, emb_file, emb_type, train):\n",
    "    # train = True\n",
    "    train_y, train_x = load_file(x_file, train)\n",
    "    train_y = adapt_labels(train_y)\n",
    "\n",
    "    avg_len, train_encoded,  t = vectorizer(train_x)\n",
    "    W = get_embeddings(emb_file, emb_type, t)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "            train_encoded, train_y, test_size=0.25, random_state=1234, shuffle=True)\n",
    "    \n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=avg_len, padding='post', truncating='post')\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=avg_len, padding='post', truncating='post')\n",
    "    \n",
    "    data = x_train, x_test, y_train, y_test \n",
    "    return data, avg_len, W, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(x_text):\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(x_text)\n",
    "    # Split by words\n",
    "    encoded = t.texts_to_sequences(x_text)\n",
    "    lengths = [len(x) for x in encoded]\n",
    "    avg_len = int(np.mean(lengths))\n",
    "    return avg_len, encoded, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "verbose = False\n",
    "directory = \"../results/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"../data/task1.train.txt\"\n",
    "emb_file = \"../data/GoogleNews-vectors-negative300.bin\"\n",
    "emb_set = \"w2v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num words found:41247\n"
     ]
    }
   ],
   "source": [
    "data, input_length, W, tokenizer = load_data(train_data, emb_file, emb_set, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = cnn()\n",
    "clf.W = W\n",
    "clf.pretrained = True\n",
    "clf.directory = directory\n",
    "clf.run(input_length, batch_size, epochs, verbose, data)\n",
    "accuracy, loss = clf.acc, clf.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(infile):\n",
    "    text = []\n",
    "    ids = []\n",
    "    for line in open(infile, \"r\"):\n",
    "        text.append(line.strip().split(\"\\t\")[0])\n",
    "        ids.append(line.strip().split(\"\\t\")[1])\n",
    "    return text, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_test = \"../data/task1.test.txt\"\n",
    "test_set, this_ids = load_test(this_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test = tokenizer.texts_to_sequences(test_set)\n",
    "padded_test = sequence.pad_sequences(encoded_test, maxlen=input_length, padding='post', truncating='post')\n",
    "this_modelf = directory + emb_set + \"_cnn.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_predictions = predict(padded_test, this_modelf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_propaganda_format(predictions, ids, name):\n",
    "    with open(name, \"w+\") as foutput:\n",
    "        for i in range(len(predictions)):\n",
    "\n",
    "            if predictions[i]:\n",
    "                prediction = 'propaganda'\n",
    "            else:\n",
    "                prediction = 'non-propaganda'\n",
    "\n",
    "            foutput.write(\"%s\\t%s\\n\" % (ids[i], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = \"../results/\" + emb_set + \"_cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_propaganda_format(this_predictions, this_ids, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
